{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "23MfWZW2QXuJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25e11347-8e49-4429-e750-7bb68cf2c613"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: music21 in /usr/local/lib/python3.12/dist-packages (9.3.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.12/dist-packages (from music21) (5.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from music21) (1.5.2)\n",
            "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.12/dist-packages (from music21) (4.1.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from music21) (3.10.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from music21) (10.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from music21) (2.32.4)\n",
            "Requirement already satisfied: webcolors>=1.5 in /usr/local/lib/python3.12/dist-packages (from music21) (24.11.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->music21) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->music21) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->music21) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->music21) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->music21) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->music21) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->music21) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->music21) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->music21) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->music21) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->music21) (2.9.0.post0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy music21 tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZuZ27pYU84_"
      },
      "outputs": [],
      "source": [
        "!wget https://storage.googleapis.com/magentadata/datasets/maestro/v3.0.0/maestro-v3.0.0-midi.zip -O /content/maestro-v3.0.0-midi.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RwlAHPmSR6cU"
      },
      "outputs": [],
      "source": [
        "!unzip /content/maestro-v3.0.0-midi.zip -d /content/maestro\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5BRd9TAQWXE",
        "outputId": "69205ab7-54dd-4648-e333-0684ce9c98d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing MIDI files...\n",
            "No MIDI files found at '/content/maestro/maestro-v3.0.0/**/*.midi'. Please check the path.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from music21 import converter, instrument, note, chord, stream\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import os\n",
        "import glob\n",
        "import pickle\n",
        "\n",
        "def get_notes(midi_files):\n",
        "    notes = []\n",
        "    total_files = len(midi_files)\n",
        "    for i, file in enumerate(midi_files):\n",
        "        print(f\"Parsing file {i+1}/{total_files}: {os.path.basename(file)}\")\n",
        "        try:\n",
        "            midi = converter.parse(file)\n",
        "            parts = instrument.partitionByInstrument(midi)\n",
        "            notes_to_parse = None\n",
        "            if parts:\n",
        "                notes_to_parse = parts.parts[0].recurse()\n",
        "            else:\n",
        "                notes_to_parse = midi.flat.notes\n",
        "\n",
        "            for element in notes_to_parse:\n",
        "                if isinstance(element, note.Note):\n",
        "                    notes.append(str(element.pitch))\n",
        "                elif isinstance(element, chord.Chord):\n",
        "                    notes.append('.'.join(str(n) for n in element.normalOrder))\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing {file}: {e}\")\n",
        "\n",
        "    with open('data/notes', 'wb') as filepath:\n",
        "        pickle.dump(notes, filepath)\n",
        "\n",
        "    return notes\n",
        "\n",
        "def prepare_sequences(notes, sequence_length=100):\n",
        "    pitch_names = sorted(set(notes))\n",
        "    n_vocab = len(pitch_names)\n",
        "\n",
        "    with open('data/pitch_names', 'wb') as filepath:\n",
        "        pickle.dump(pitch_names, filepath)\n",
        "\n",
        "    note_to_int = {note: number for number, note in enumerate(pitch_names)}\n",
        "\n",
        "    network_input = []\n",
        "    network_output = []\n",
        "\n",
        "    for i in range(0, len(notes) - sequence_length):\n",
        "        sequence_in = notes[i:i + sequence_length]\n",
        "        sequence_out = notes[i + sequence_length]\n",
        "        network_input.append([note_to_int[char] for char in sequence_in])\n",
        "        network_output.append(note_to_int[sequence_out])\n",
        "\n",
        "    n_patterns = len(network_input)\n",
        "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
        "    network_input = network_input / float(n_vocab)\n",
        "    network_output = to_categorical(network_output, num_classes=n_vocab)\n",
        "\n",
        "    return network_input, network_output\n",
        "\n",
        "def create_model(network_input, n_vocab):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(512, input_shape=(network_input.shape[1], network_input.shape[2]), return_sequences=True))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(LSTM(512))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(n_vocab, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "def train_model(network_input, network_output, n_vocab):\n",
        "    model = create_model(network_input, n_vocab)\n",
        "\n",
        "    # NEW LINE\n",
        "    filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}.keras\"\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        filepath,\n",
        "        monitor='loss',\n",
        "        verbose=0,\n",
        "        save_best_only=True,\n",
        "        mode='min'\n",
        "    )\n",
        "    callbacks_list = [checkpoint]\n",
        "\n",
        "    model.fit(network_input, network_output, epochs=50, batch_size=128, callbacks=callbacks_list, verbose=1)\n",
        "    return model\n",
        "\n",
        "def main():\n",
        "    DATASET_PATH = \"/content/maestro/maestro-v3.0.0/**/*.midi\"\n",
        "    MAX_FILES_TO_PROCESS = 10\n",
        "\n",
        "    if not os.path.exists('data'):\n",
        "        os.makedirs('data')\n",
        "\n",
        "    if os.path.exists('data/notes'):\n",
        "        print(\"Loading notes from cache...\")\n",
        "        with open('data/notes', 'rb') as filepath:\n",
        "            notes = pickle.load(filepath)\n",
        "    else:\n",
        "        print(\"Parsing MIDI files...\")\n",
        "        midi_files = glob.glob(DATASET_PATH, recursive=True)\n",
        "        if not midi_files:\n",
        "            print(f\"No MIDI files found at '{DATASET_PATH}'. Please check the path.\")\n",
        "            return\n",
        "\n",
        "        midi_files = midi_files[:MAX_FILES_TO_PROCESS]\n",
        "\n",
        "        notes = get_notes(midi_files)\n",
        "\n",
        "    if not notes:\n",
        "        print(\"No notes extracted. Aborting.\")\n",
        "        return\n",
        "\n",
        "    n_vocab = len(set(notes))\n",
        "    print(f\"Total notes parsed: {len(notes)}\")\n",
        "    print(f\"Vocabulary size (unique notes/chords): {n_vocab}\")\n",
        "\n",
        "    sequence_length = 10\n",
        "    network_input, network_output = prepare_sequences(notes, sequence_length)\n",
        "\n",
        "    print(\"Building and training model...\")\n",
        "    train_model(network_input, network_output, n_vocab)\n",
        "\n",
        "    print(\"\\nTraining complete. To generate music, run a separate generation script using the saved weights.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9MQrtRFTxxY",
        "outputId": "d2416632-1267-45c7-bfd8-8f16999d864d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data for music generation...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/notes'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4269377668.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4269377668.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading data for music generation...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/notes'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mnotes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/pitch_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/notes'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from music21 import converter, instrument, note, chord, stream\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "import pickle\n",
        "import glob\n",
        "import os\n",
        "\n",
        "def create_model(network_input, n_vocab):\n",
        "    \"\"\" Recreate the exact same model architecture as in training \"\"\"\n",
        "    model = Sequential([\n",
        "        LSTM(512, input_shape=(network_input.shape[1], network_input.shape[2]), return_sequences=True),\n",
        "        Dropout(0.3),\n",
        "        LSTM(512),\n",
        "        Dropout(0.3),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dense(n_vocab, activation='softmax')\n",
        "    ])\n",
        "    # No need to compile for prediction, but it's good practice\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "def generate_notes(model, network_input, pitch_names, n_vocab, num_notes=200):\n",
        "    \"\"\" Generate notes from the neural network based on a random seed \"\"\"\n",
        "    start = np.random.randint(0, len(network_input)-1)\n",
        "    int_to_note = {number: note for number, note in enumerate(pitch_names)}\n",
        "\n",
        "    # Use a slice of the original input data as the starting pattern\n",
        "    pattern_indices = network_input[start]\n",
        "    prediction_output = []\n",
        "\n",
        "    # --- THIS IS THE FIX ---\n",
        "    # Convert the list to a numpy array before performing division\n",
        "    pattern = np.array(pattern_indices) / float(n_vocab)\n",
        "\n",
        "    for note_index in range(num_notes):\n",
        "        prediction_input = np.reshape(pattern, (1, len(pattern), 1))\n",
        "        prediction = model.predict(prediction_input, verbose=0)\n",
        "\n",
        "        index = np.argmax(prediction)\n",
        "        result = int_to_note[index]\n",
        "        prediction_output.append(result)\n",
        "\n",
        "        # Update the pattern: remove the first note and add the new prediction\n",
        "        new_index_normalized = index / float(n_vocab)\n",
        "        pattern = np.append(pattern[1:], new_index_normalized)\n",
        "\n",
        "    return prediction_output\n",
        "\n",
        "def create_midi(prediction_output, output_file='ai_generated_music.mid'):\n",
        "    \"\"\" Convert the output from the prediction to notes and create a midi file \"\"\"\n",
        "    offset = 0\n",
        "    output_notes = []\n",
        "    for pattern in prediction_output:\n",
        "        # It's a chord\n",
        "        if ('.' in pattern) or pattern.isdigit():\n",
        "            notes_in_chord = pattern.split('.')\n",
        "            chord_notes = []\n",
        "            for current_note in notes_in_chord:\n",
        "                new_note = note.Note(int(current_note))\n",
        "                new_note.storedInstrument = instrument.Piano()\n",
        "                chord_notes.append(new_note)\n",
        "            new_chord = chord.Chord(chord_notes)\n",
        "            new_chord.offset = offset\n",
        "            output_notes.append(new_chord)\n",
        "        # It's a note\n",
        "        else:\n",
        "            new_note = note.Note(pattern)\n",
        "            new_note.offset = offset\n",
        "            new_note.storedInstrument = instrument.Piano()\n",
        "            output_notes.append(new_note)\n",
        "        # Increment offset for the next note/chord\n",
        "        offset += 0.5\n",
        "\n",
        "    midi_stream = stream.Stream(output_notes)\n",
        "    midi_stream.write('midi', fp=output_file)\n",
        "    print(f\"Successfully saved generated music to {output_file}\")\n",
        "\n",
        "def main():\n",
        "    print(\"Loading data for music generation...\")\n",
        "    with open('data/notes', 'rb') as filepath:\n",
        "        notes = pickle.load(filepath)\n",
        "    with open('data/pitch_names', 'rb') as filepath:\n",
        "        pitch_names = pickle.load(filepath)\n",
        "\n",
        "    n_vocab = len(set(notes))\n",
        "    sequence_length = 100 # Must be the same as in training\n",
        "\n",
        "    # Prepare sequences for seeding the model (we only need the input part)\n",
        "    note_to_int = {note: number for number, note in enumerate(pitch_names)}\n",
        "    network_input = []\n",
        "    for i in range(len(notes) - sequence_length):\n",
        "        sequence_in = notes[i:i + sequence_length]\n",
        "        network_input.append([note_to_int[char] for char in sequence_in])\n",
        "\n",
        "    # Find the best weights file\n",
        "    weight_files = glob.glob(\"weights-improvement-*.keras\")\n",
        "    if not weight_files:\n",
        "        print(\"Error: No weights files found. Please run train.py first.\")\n",
        "        return\n",
        "\n",
        "    # Sort files by epoch number to get the latest one\n",
        "    latest_weights_file = max(weight_files, key=os.path.getctime)\n",
        "    print(f\"Loading weights from: {latest_weights_file}\")\n",
        "\n",
        "    # Recreate the model and load the weights\n",
        "    model_input_shape = (len(network_input[0]), 1) # (sequence_length, 1)\n",
        "    model = create_model(np.zeros((1, *model_input_shape)), n_vocab)\n",
        "    model.load_weights(latest_weights_file)\n",
        "\n",
        "    print(\"Generating new music...\")\n",
        "    prediction_output = generate_notes(model, network_input, pitch_names, n_vocab)\n",
        "\n",
        "    print(\"Converting generated notes to MIDI file...\")\n",
        "    create_midi(prediction_output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZnZvQ1hVoKb"
      },
      "outputs": [],
      "source": [
        "# Install the synthesizer program (example for Linux/Colab)\n",
        "!sudo apt-get install fluidsynth\n",
        "!pip install midi2audio\n",
        "# Install the Python library\n",
        "!pip install pyfluidsynth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLyyT86gVlmt"
      },
      "outputs": [],
      "source": [
        "# Add this import at the top\n",
        "from midi2audio import FluidSynth\n",
        "\n",
        "# After calling create_midi(), add this:\n",
        "print(\"Converting MIDI to audio...\")\n",
        "fs = FluidSynth('path/to/your/GeneralUser_GS_v1.471.sf2') # Use the path to your .sf2 file\n",
        "fs.midi_to_audio('ai_generated_music.mid', 'ai_generated_music.wav')\n",
        "print(\"Audio file ai_generated_music.wav saved successfully!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}